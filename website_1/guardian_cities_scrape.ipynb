{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from website_1.items import Website1Item\n",
    "import re\n",
    "import datetime \n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "month = ''\n",
    "\n",
    "if str(now.month) not in ['10','11','12']:\n",
    "    month = '0'+str(now.month)\n",
    "\n",
    "else:\n",
    "    month = str(now.month)\n",
    "today = str(now.year)+'/'+month+'/'+str(now.day)\n",
    "\n",
    "class Website1(scrapy.Spider):\n",
    "\n",
    "    name = \"cities_scraper\"\n",
    "\n",
    "    # First Start Url\n",
    "\n",
    "    guardian_url = ['https://www.theguardian.com/']\n",
    "    start_urls = []\n",
    "    topics = ['technology', 'environment', 'science', 'sport', 'law', 'politics', 'society', 'global-development', 'cities', 'culture']\n",
    "\n",
    "    start_urls.append('https://www.theguardian.com/'+'cities'+'/'+today)\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        for href in response.xpath(\"//a[contains(@class, 'u-faux-block-link__overlay js-headline-text')]//@href\"):\n",
    "\n",
    "            url  = href.extract() \n",
    "\n",
    "            yield scrapy.Request(url, callback=self.parse_dir_contents)\n",
    "\n",
    "    def parse_dir_contents(self, response):\n",
    "\n",
    "        item = Website1Item()\n",
    "\n",
    "        # Getting Article Title\n",
    "\n",
    "        item['title'] = response.xpath(\"//title/descendant::text()\").extract()[0].strip()\n",
    "\n",
    "        # Getting Article Description\n",
    "\n",
    "        item['description']= response.xpath(\"//meta[contains(@name, 'description')]//@content\").extract()[0]\n",
    "\n",
    "        # First Published\n",
    "\n",
    "        item['date'] = response.xpath(\"//time[contains(@itemprop, 'datePublished')]/@datetime\").extract()[0][:10]\n",
    "\n",
    "        # Url (The link to the page)\n",
    "\n",
    "        item['url'] = response.xpath(\"//meta[@property='og:url']/@content\").extract()\n",
    "\n",
    "        yield item\n",
    "\n",
    "# In terminal use scrapy crawl sport_scraper -o Sport.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
